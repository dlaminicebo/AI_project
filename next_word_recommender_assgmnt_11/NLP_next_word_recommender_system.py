# -*- coding: utf-8 -*-
"""10.6 Next word Recommender System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LjbO_C5olCdSs5Da_Spigo_cMvvEfFwr

## Steps to build the next word recommender system

1. Loading and exploring the dataset
2. Creating N-grams of the dialogue
3. Building the N-gram Language Model
4. Predicting the next word using N-gram Language Model

## 1. Loading and exploring the dataset
"""

# loading the required libraries
import pandas as pd
import numpy as np
import re
import pickle
import random
from tqdm import tqdm

# mounting the drive
from google.colab import drive
drive.mount('/content/drive')

# open text file and read in data
#with open("drive/My Drive/dialogs_dataset", "rb") as f:
with open("drive/MyDrive/Colab Notebooks/Project - Next  Word Recommender System/dialogs_dataset", "rb") as f:
    dialogs = pickle.load(f)

# number of text sequences
len(dialogs)

# print 10 random dialogs
random.sample(dialogs, 10)

# text cleaning
dialogs_clean = []

for i in dialogs:
  # remove everything except alphabets, ' and white spaces
  i = re.sub("[^a-zA-Z' ]", "", i)
  # convert text to lowercase
  i = i.lower()
  # add cleaned text to the list
  dialogs_clean.append(i)

random.sample(dialogs_clean, 10)

# creating the vocabulary
# get list of all the words
all_words = " ".join(dialogs_clean).split()

words_dict = {}

# add word-count pair to the dictionary
for word in all_words:
    # check if the word is already in dictionary
    if word in words_dict:
        # increment count of word by 1
        words_dict[word] = words_dict[word] + 1
    else:
        # add the word to dictionary with count 1
        words_dict[word] = 1

# word dictionary
words_dict

# prepare a dataframe
words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})

# sort words by their count in increasing order
words_df = words_df.sort_values(by = ['count'])

# reset dataframe index
words_df.reset_index(inplace = True, drop=True)

# words with least frequency
words_df.head()

# words with highest frequency
words_df.tail()

# vocabulary size
len(words_df)

"""## 2. Creating N-grams of the dialogue"""

# creating an empty dataframe
dataset = pd.DataFrame()

# adding cleaned sentences in the dataframe
dataset['Sentences'] = dialogs_clean

# first 20 cleaned sentences
dataset.head(20)

# using .split() to get tokens from the sentence
dataset['Sentences'][0].split()

# function to create unigrams
# taking a sentence as input
def create_unigram(sentence):
    # creating tokens from the sentence
    tokens = sentence.split()
    # empty list to store the unigrams
    unigram_list = []
    # number of unigrams is equal to the number of tokens in the sentence
    for i in range(len(tokens)):
        # appending each unigram in the list
        unigram_list.append(tokens[i:i+1])
    # returning the unigram list for a sentence
    return unigram_list

# function to create bigrams
def create_bigram(sentence):
    tokens = sentence.split()
    bigram_list = []
    # number of bigrams is one less than the number of tokens in the sentence
    for i in range(len(tokens)-1):
        bigram_list.append(tokens[i:i+2])
        #print(tokens[i:i+2])
    return bigram_list

# function to create trigrams
def create_trigram(sentence):
    tokens = sentence.split()
    trigram_list = []
    # number of trigrams is two less than the number of tokens in the sentence
    for i in range(len(tokens)-2):
        trigram_list.append(tokens[i:i+3])
    return trigram_list

# creating unigrams for all the sentences in the dataset
final_unigram = []
# for each sentence
for i in range(dataset.shape[0]):
    # using the defined unigram function to create unigrams
    final_unigram.append(create_unigram(dataset['Sentences'][i]))

# adding the unigram in a seperate column in the dataset
dataset['unigram'] = final_unigram

# creating bigrams for all the sentences in the dataset
final_bigram = []
for i in range(dataset.shape[0]):
    final_bigram.append(create_bigram(dataset['Sentences'][i]))

dataset['bigram'] = final_bigram

# creating trigrams for all the sentences in the dataset
final_trigram = []
for i in range(dataset.shape[0]):
    final_trigram.append(create_trigram(dataset['Sentences'][i]))

dataset['trigram'] = final_trigram

# first 20 rows of the dataset
dataset.head(20)

# sample sentence
dataset['Sentences'][0]

# unigram of the sentence
dataset['unigram'][0]

# bigram of the sentence
dataset['bigram'][0]

# trigram of the sentence
dataset['trigram'][0]

"""## 3. Building the N-gram Language Model"""

# for defining the N-gram model
from collections import Counter, defaultdict

# Create a placeholder for model
model = defaultdict(lambda: defaultdict(lambda: 0))

# Count frequency of co-occurance
for i in range(dataset.shape[0]):
    # for each trigram pair
    for w1, w2, w3 in create_trigram(dataset['Sentences'][i]):
        # count the occurance of word 3, given word 1 and word 2
        model[(w1, w2)][w3] += 1

# defined model
model

"""## 4. Predicting the next word using N-gram Language Model"""

# predict the next word
dict(model["to", "book"])

# another example
dict(model["my", "name"])

# another example
dict(model["how", "are"])

# another example
dict(model["good", "to"])

"""### Probabilistic Output"""

# creating the unigram list
unigram_dict = {}
for i in tqdm(range(dataset.shape[0])):
    # add word-count pair to the dictionary
    for word in dataset['unigram'][i]:
        # check if the word is already in dictionary
        if word[0] in unigram_dict:
            # increment count of word by 1
            unigram_dict[word[0]] = unigram_dict[word[0]] + 1
        else:
            # add the word to dictionary with count 1
            unigram_dict[word[0]] = 1

# unigram list
unigram_dict

# find the overall frequency of words in the corpus
counts = Counter(unigram_dict)
counts

# vocabulary size
total_count = len(unigram_dict)
total_count

# relative frequencies of each word
for word in counts:
    counts[word] /= float(total_count)

counts

# Let's transform the counts to probabilities
for w1_w2 in model:
    total_count = float(sum(model[w1_w2].values()))
    for w3 in model[w1_w2]:
        model[w1_w2][w3] /= total_count

# predict the next word
dict(model["to", "book"])

# another example
dict(model["how", "are"])

# another example
dict(model["good", "to"])

