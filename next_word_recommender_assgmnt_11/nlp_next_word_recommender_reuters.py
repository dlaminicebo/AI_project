# -*- coding: utf-8 -*-
"""nlp_next_word_recommender_reuters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19PQoBWb85FXLrQejQnfS8_8yqdNLReQY
"""

# loading the required libraries
import pandas as pd
import numpy as np
import re
import pickle
import random
from tqdm import tqdm

pd.set_option('display.width', 500)

# mounting the drive
from google.colab import drive
drive.mount('/content/drive')

def print_df(df):
    print(df.head())
    print(df.shape)
    print(df.info())

# open text file and read in data
#with open("drive/My Drive/dialogs_dataset", "rb") as f:
#with open("drive/MyDrive/Colab Notebooks/Project - Next  Word Recommender System/dialogs_dataset", "rb") as f:
#    dialogs = pickle.load(f)

df_reuters = pd.read_csv("drive/MyDrive/Colab Notebooks/Project - Next  Word Recommender System/sample_reuters_dataset.csv")
print_df(df_reuters)

# text cleaning
def clean_data(text):
    # remove everything except alphabets, ' and white spaces
    text = re.sub("[^a-zA-Z' ]", "", text)
    # convert text to lowercase
    text = text.lower()
    return text

df_reuters["clean_sentences"] = df_reuters['sentence_text'].apply(clean_data)
print_df(df_reuters)

# creating the vocabulary
# get list of all the words
all_words = " ".join(df_reuters['clean_sentences']).split()
print(type(all_words))

words_dict = {}

# add word-count pair to the dictionary
for word in all_words:
    # check if the word is already in dictionary
    if word in words_dict:
        # increment count of word by 1
        words_dict[word] = words_dict[word] + 1
    else:
        # add the word to dictionary with count 1
        words_dict[word] = 1

print(words_dict)

# prepare a dataframe
words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})

# sort words by their count in increasing order
words_df = words_df.sort_values(by = ['count'])

# reset dataframe index
words_df.reset_index(inplace = True, drop=True)

# words with least frequency
print(words_df.head())

# words with highest frequency
print(words_df.tail())

# vocabulary size
print("vocabulary size:", len(words_df))

# creating an empty dataframe
dataset = pd.DataFrame()

# adding cleaned sentences in the dataframe
dataset['Sentences'] = df_reuters['clean_sentences']

# first 20 cleaned sentences
print(dataset.head(10))

# using .split() to get tokens from the sentence
dataset['Sentences'][0].split()

# function to create unigrams
# taking a sentence as input
def create_unigram(sentence):
    # creating tokens from the sentence
    tokens = sentence.split()
    # empty list to store the unigrams
    unigram_list = []
    # number of unigrams is equal to the number of tokens in the sentence
    for i in range(len(tokens)):
        # appending each unigram in the list
        unigram_list.append(tokens[i:i+1])
    # returning the unigram list for a sentence
    return unigram_list

# function to create bigrams
def create_bigram(sentence):
    tokens = sentence.split()
    bigram_list = []
    # number of bigrams is one less than the number of tokens in the sentence
    for i in range(len(tokens)-1):
        bigram_list.append(tokens[i:i+2])
        #print(tokens[i:i+2])
    return bigram_list

# function to create trigrams
def create_trigram(sentence):
    tokens = sentence.split()
    trigram_list = []
    # number of trigrams is two less than the number of tokens in the sentence
    for i in range(len(tokens)-2):
        trigram_list.append(tokens[i:i+3])
    return trigram_list

# creating unigrams for all the sentences in the dataset
final_unigram = []
# for each sentence
for i in range(dataset.shape[0]):
    # using the defined unigram function to create unigrams
    final_unigram.append(create_unigram(dataset['Sentences'][i]))

# adding the unigram in a seperate column in the dataset
dataset['unigram'] = final_unigram

# creating bigrams for all the sentences in the dataset
final_bigram = []
for i in range(dataset.shape[0]):
    final_bigram.append(create_bigram(dataset['Sentences'][i]))

dataset['bigram'] = final_bigram

# creating trigrams for all the sentences in the dataset
final_trigram = []
for i in range(dataset.shape[0]):
    final_trigram.append(create_trigram(dataset['Sentences'][i]))

dataset['trigram'] = final_trigram

# first 20 rows of the dataset
dataset.head(20)

# sample sentence
print(dataset['Sentences'][0])
print(dataset['Sentences'][1])
print(dataset['Sentences'][2])

# unigram of the sentence
print("unigram", dataset['unigram'][0])
print("unigram", dataset['unigram'][1])

# bigram of the sentence
print("bigram", dataset['bigram'][0])
print("bigram", dataset['bigram'][1])

# trigram of the sentence
print("trigram", dataset['trigram'][0])
print("trigram", dataset['trigram'][1])

# for defining the N-gram model
from collections import Counter, defaultdict

# Create a placeholder for model
model = defaultdict(lambda: defaultdict(lambda: 0))

# Count frequency of co-occurance
for i in range(dataset.shape[0]):
    # for each trigram pair
    for w1, w2, w3 in create_trigram(dataset['Sentences'][i]):
        # count the occurance of word 3, given word 1 and word 2
        model[(w1, w2)][w3] += 1

print(dict(model["they", "told"]))
print(dict(model["they", "are"]))

# creating the unigram list
unigram_dict = {}
for i in tqdm(range(dataset.shape[0])):
    # add word-count pair to the dictionary
    for word in dataset['unigram'][i]:
        # check if the word is already in dictionary
        if word[0] in unigram_dict:
            # increment count of word by 1
            unigram_dict[word[0]] = unigram_dict[word[0]] + 1
        else:
            # add the word to dictionary with count 1
            unigram_dict[word[0]] = 1

unigram_dict

# find the overall frequency of words in the corpus
counts = Counter(unigram_dict)
counts

# vocabulary size
total_count = len(unigram_dict)
total_count

# relative frequencies of each word
for word in counts:
    counts[word] /= float(total_count)

counts

# Let's transform the counts to probabilities
for w1_w2 in model:
    total_count = float(sum(model[w1_w2].values()))
    for w3 in model[w1_w2]:
        model[w1_w2][w3] /= total_count

# predict the next word
dict(model["they", "told"])

# another example
dict(model["they", "are"])

